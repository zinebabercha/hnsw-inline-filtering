{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hnswlib\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### HNSW Filtering Benchmark Notebook\n",
    "\n",
    "This notebook performs benchmarking of inline HNSW (Hierarchical Navigable Small World) filtering performance using two approaches:\n",
    "\n",
    "##### 1. Uniform Distribution (`LabelFilteredANNEvaluator`)\n",
    "- Tests filtering on uniformly distributed labels (a, b, c)\n",
    "- Equal distribution (~33.3% each)\n",
    "\n",
    "##### 2. Skewed Distribution (`LabelFilteredANNEvaluator1`)\n",
    "- Same evaluation but with skewed label distribution:\n",
    "  - Label a: 60%\n",
    "  - Label b: 30%\n",
    "  - Label c: 10%\n",
    "\n",
    "##### Key Metrics Evaluated\n",
    "- Build Time\n",
    "- Query Latency (filtered vs unfiltered)\n",
    "- Filter Specificity\n",
    "- Recall Scores\n",
    "- Filter Friction (Latency Overhead and Recall Impact)\n",
    "\n",
    "The inline filtering function takes Id and returns True or False.\n",
    "\n",
    "Results are printed showing performance metrics for both distribution strategies, allowing comparison of filtering effectiveness under different data distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelFilteredANNEvaluator:\n",
    "    \"\"\"\n",
    "    Design Metrics for Filtered ANN Search:\n",
    "    1. Query Latency:\n",
    "       - Measures search time with/without filters\n",
    "       - Compares overhead of filtering\n",
    "    2. Accuracy Impact:\n",
    "       - Recall@k: proportion of true nearest neighbors found\n",
    "       - How filtering affects quality of results\n",
    "    3. Filter Friction:\n",
    "       - Filter specificity: proportion of points passing filter\n",
    "       - Impact of label distribution on performance\n",
    "    \"\"\"\n",
    "    def __init__(self, dim=16, num_elements=3000):\n",
    "        self.dim = dim\n",
    "        self.num_elements = num_elements\n",
    "        self.metrics = defaultdict(list)\n",
    "\n",
    "    def generate_uniform_labeled_data(self):\n",
    "        \"\"\"Generate uniform data with three labels distributed equally\"\"\"\n",
    "        self.data = np.float32(np.random.random((self.num_elements, self.dim)))\n",
    "        \n",
    "        num_per_label = self.num_elements // 3\n",
    "        self.labels = np.array(['a'] * num_per_label + \n",
    "                             ['b'] * num_per_label + \n",
    "                             ['c'] * (self.num_elements - 2 * num_per_label))\n",
    "        p = np.random.permutation(len(self.data))\n",
    "        self.data = self.data[p]\n",
    "        self.labels = self.labels[p]\n",
    "        unique, counts = np.unique(self.labels, return_counts=True)\n",
    "        self.metrics['label_distribution'] = dict(zip(unique, counts / len(self.labels)))\n",
    "        return self.data, self.labels\n",
    " \n",
    "    def build_index(self):\n",
    "        \"\"\"Build HNSW index with the generated data\"\"\"\n",
    "        self.index = hnswlib.Index(space='cosine', dim=self.dim)\n",
    "        self.index.init_index(max_elements=self.num_elements, ef_construction=100, M=16)\n",
    "        self.index.set_ef(20)\n",
    "        self.index.set_num_threads(1)\n",
    "        start_time = time.time()\n",
    "        self.index.add_items(self.data, ids=np.arange(self.num_elements))\n",
    "        build_time = time.time() - start_time\n",
    "        self.metrics['build_time'] = build_time\n",
    "\n",
    "    def create_label_filter(self, target_label):\n",
    "        \"\"\"Create filter function for a specific label\"\"\"\n",
    "        def filter_function(idx):\n",
    "            return self.labels[idx] == target_label\n",
    "        return filter_function\n",
    "    \n",
    "    def calculate_recall(self, filtered_results, true_results, query_points, target_label, k):\n",
    "        \"\"\"\n",
    "    Calculate recall@k for filtered nearest neighbor search results.\n",
    "    \n",
    "    Args:\n",
    "        filtered_results: Results from filtered knn search (n_queries x k)\n",
    "        true_results: Results from unfiltered knn search (n_queries x k)\n",
    "        query_points: Query points used for search (n_queries x dim)\n",
    "        target_label: Label to filter for\n",
    "        k: Number of nearest neighbors\n",
    "    \n",
    "    Returns:\n",
    "        float: Average recall@k across all queries\n",
    "        \"\"\"\n",
    "        recall = 0\n",
    "        n_queries = len(query_points)\n",
    "    \n",
    "        target_mask = self.labels == target_label\n",
    "        target_data = self.data[target_mask]\n",
    "        target_indices = np.where(target_mask)[0]\n",
    "    \n",
    "        for i in range(n_queries):\n",
    "            distances = np.linalg.norm(target_data - query_points[i], axis=1)\n",
    "            true_neighbor_indices = target_indices[np.argsort(distances)[:k]]\n",
    "        \n",
    "            filtered_neighbor_indices = filtered_results[i]\n",
    "        \n",
    "            intersection = set(filtered_neighbor_indices) & set(true_neighbor_indices)\n",
    "            recall += len(intersection) / k\n",
    "    \n",
    "        return recall / n_queries\n",
    "\n",
    "    def evaluate_query_performance(self, num_queries=100, k=10):\n",
    "        \"\"\"Evaluate query performance with comprehensive metrics\"\"\"\n",
    "\n",
    "        query_points = np.float32(np.random.random((num_queries, self.dim)))\n",
    "        \n",
    "        #  Unfiltered\n",
    "        start_time = time.time()\n",
    "        unfiltered_labels, unfiltered_distances = self.index.knn_query(query_points, k=k, num_threads=1)\n",
    "        unfiltered_time = time.time() - start_time\n",
    "        \n",
    "        filter_times = {}\n",
    "        recall_scores = {}\n",
    "        filter_specificity = {}\n",
    "        result_counts = {}\n",
    "        \n",
    "        # Per each label metrics\n",
    "        for label in ['a', 'b', 'c']:\n",
    "            filter_func = self.create_label_filter(label)\n",
    "            \n",
    "            # Latency\n",
    "            start_time = time.time()\n",
    "            filtered_labels, filtered_distances = self.index.knn_query(\n",
    "                query_points, k=k, num_threads=1, filter=filter_func\n",
    "            )\n",
    "            filter_time = time.time() - start_time\n",
    "            filter_times[label] = filter_time / num_queries\n",
    "            # total_requested = len(self.labels)\n",
    "            # matching_label = np.sum(self.labels == label)\n",
    "            # result_counts[label] = matching_label / total_requested\n",
    "            \n",
    "            # Filter Specificity\n",
    "            points_passing_filter = sum(filter_func(i) for i in range(self.num_elements))\n",
    "            filter_specificity[label] = points_passing_filter / self.num_elements\n",
    "            \n",
    "            # Accuracy Impact (Recall)\n",
    "            recall_scores[label] = self.calculate_recall(\n",
    "    filtered_labels, \n",
    "    unfiltered_labels,\n",
    "    query_points, \n",
    "    label,         \n",
    "    k              \n",
    ")\n",
    "\n",
    "        self.metrics['query_latency'] = {\n",
    "            'unfiltered': unfiltered_time / num_queries,\n",
    "            'filtered': filter_times\n",
    "        }\n",
    "        # self.metrics['filter_specificity'] = result_counts\n",
    "        self.metrics['filter_specificity'] = filter_specificity\n",
    "        self.metrics['recall_scores'] = recall_scores\n",
    "        self.metrics['filter_friction'] = {\n",
    "            'latency_overhead': {label: filter_times[label]/self.metrics['query_latency']['unfiltered'] \n",
    "                        for label in filter_times},\n",
    "            'specificity': filter_specificity,\n",
    "            'recall_impact': recall_scores\n",
    "}\n",
    "        \n",
    "        return self.metrics\n",
    "    \n",
    "    # def evaluate_query_performance(self, num_queries=100, k=10):\n",
    "    #     \"\"\"Evaluate query performance with different label filters\"\"\"\n",
    "    #     query_points = np.float32(np.random.random((num_queries, self.dim)))\n",
    "        \n",
    "    #     start_time = time.time()\n",
    "    #     unfiltered_labels, unfiltered_distances = self.index.knn_query(query_points, k=k, num_threads=1)\n",
    "    #     unfiltered_time = time.time() - start_time\n",
    "        \n",
    "    #     filter_times = {}\n",
    "    #     result_counts = {}\n",
    "    #     filter_specificity = {}\n",
    "\n",
    "        \n",
    "    #     for label in ['a', 'b', 'c']:\n",
    "    #         filter_func = self.create_label_filter(label)\n",
    "            \n",
    "    #         start_time = time.time()\n",
    "    #         filtered_labels, filtered_distances = self.index.knn_query(\n",
    "    #             query_points, k=k, num_threads=1, filter=filter_func\n",
    "    #         )\n",
    "    #         filter_time = time.time() - start_time\n",
    "            \n",
    "    #         filter_times[label] = filter_time / num_queries\n",
    "    #         total_requested = len(self.labels)\n",
    "    #         matching_label = np.sum(self.labels == label)\n",
    "    #         result_counts[label] = matching_label / total_requested\n",
    "    #         # Count how many points pass the filter\n",
    "    #         points_passing_filter = sum(filter_func(i) for i in range(self.num_elements))\n",
    "    #         filter_specificity[label] = points_passing_filter / self.num_elements\n",
    "        \n",
    "    #     self.metrics['query_latency'] = {\n",
    "    #         'unfiltered': unfiltered_time / num_queries,\n",
    "    #         'filtered': filter_times\n",
    "    #     }\n",
    "    #     # self.metrics['filter_specificity'] = result_counts\n",
    "\n",
    "    \n",
    "    #     self.metrics['filter_specificity'] = filter_specificity\n",
    "        \n",
    "    #     return self.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelFilteredANNEvaluator1:\n",
    "    \"\"\"\n",
    "    Design Metrics for Filtered ANN Search:\n",
    "    1. Query Latency:\n",
    "       - Measures search time with/without filters\n",
    "       - Compares overhead of filtering\n",
    "    2. Accuracy Impact:\n",
    "       - Recall@k: proportion of true nearest neighbors found\n",
    "       - How filtering affects quality of results\n",
    "    3. Filter Friction:\n",
    "       - Filter specificity: proportion of points passing filter\n",
    "       - Impact of label distribution on performance\n",
    "    \"\"\"\n",
    "    def __init__(self, dim=16, num_elements=3000):\n",
    "        self.dim = dim\n",
    "        self.num_elements = num_elements\n",
    "        self.metrics = defaultdict(list)\n",
    "\n",
    "   \n",
    "    def generate_skewed_labeled_data(self):\n",
    "        \"\"\"Generate skewed data with three labels distributed as 60%, 30%, 10%\"\"\"\n",
    "        self.data = np.float32(np.random.random((self.num_elements, self.dim)))\n",
    "    \n",
    "        label_a_count = int(self.num_elements * 0.6)  # 60%\n",
    "        label_b_count = int(self.num_elements * 0.3)  # 30%\n",
    "        label_c_count = self.num_elements - label_a_count - label_b_count  # Remaining (10%)\n",
    "    \n",
    "        self.labels = np.array(['a'] * label_a_count + \n",
    "                          ['b'] * label_b_count + \n",
    "                          ['c'] * label_c_count)\n",
    "    \n",
    "        p = np.random.permutation(len(self.data))\n",
    "        self.data = self.data[p]\n",
    "        self.labels = self.labels[p]\n",
    "    \n",
    "        unique, counts = np.unique(self.labels, return_counts=True)\n",
    "        self.metrics['label_distribution'] = dict(zip(unique, counts / len(self.labels)))\n",
    "    \n",
    "        return self.data, self.labels\n",
    "    \n",
    "    def build_index(self):\n",
    "        \"\"\"Build HNSW index with the generated data\"\"\"\n",
    "        self.index = hnswlib.Index(space='cosine', dim=self.dim)\n",
    "        self.index.init_index(max_elements=self.num_elements, ef_construction=100, M=16)\n",
    "        self.index.set_ef(20)\n",
    "        self.index.set_num_threads(1)\n",
    "        start_time = time.time()\n",
    "        self.index.add_items(self.data, ids=np.arange(self.num_elements))\n",
    "        build_time = time.time() - start_time\n",
    "        self.metrics['build_time'] = build_time\n",
    "\n",
    "    def create_label_filter(self, target_label):\n",
    "        \"\"\"Create filter function for a specific label\"\"\"\n",
    "        def filter_function(idx):\n",
    "            return self.labels[idx] == target_label\n",
    "        return filter_function\n",
    "    \n",
    "    def calculate_recall(self, filtered_results, true_results, query_points, target_label, k):\n",
    "        \"\"\"\n",
    "    Calculate recall@k for filtered nearest neighbor search results.\n",
    "    \n",
    "    Args:\n",
    "        filtered_results: Results from filtered knn search (n_queries x k)\n",
    "        true_results: Results from unfiltered knn search (n_queries x k)\n",
    "        query_points: Query points used for search (n_queries x dim)\n",
    "        target_label: Label to filter for\n",
    "        k: Number of nearest neighbors\n",
    "    \n",
    "    Returns:\n",
    "        float: Average recall@k across all queries\n",
    "        \"\"\"\n",
    "        recall = 0\n",
    "        n_queries = len(query_points)\n",
    "    \n",
    "        target_mask = self.labels == target_label\n",
    "        target_data = self.data[target_mask]\n",
    "        target_indices = np.where(target_mask)[0]\n",
    "    \n",
    "        for i in range(n_queries):\n",
    "            distances = np.linalg.norm(target_data - query_points[i], axis=1)\n",
    "            true_neighbor_indices = target_indices[np.argsort(distances)[:k]]\n",
    "        \n",
    "            filtered_neighbor_indices = filtered_results[i]\n",
    "        \n",
    "            intersection = set(filtered_neighbor_indices) & set(true_neighbor_indices)\n",
    "            recall += len(intersection) / k\n",
    "    \n",
    "        return recall / n_queries\n",
    "\n",
    "    def evaluate_query_performance(self, num_queries=100, k=10):\n",
    "        \"\"\"Evaluate query performance with comprehensive metrics\"\"\"\n",
    "\n",
    "        query_points = np.float32(np.random.random((num_queries, self.dim)))\n",
    "        \n",
    "        # Unfiltered\n",
    "        start_time = time.time()\n",
    "        unfiltered_labels, unfiltered_distances = self.index.knn_query(query_points, k=k, num_threads=1)\n",
    "        unfiltered_time = time.time() - start_time\n",
    "        \n",
    "        filter_times = {}\n",
    "        recall_scores = {}\n",
    "        filter_specificity = {}\n",
    "        result_counts = {}\n",
    "        \n",
    "        # Per each label metrics\n",
    "        for label in ['a', 'b', 'c']:\n",
    "            filter_func = self.create_label_filter(label)\n",
    "            \n",
    "            # Latency\n",
    "            start_time = time.time()\n",
    "            filtered_labels, filtered_distances = self.index.knn_query(\n",
    "                query_points, k=k, num_threads=1, filter=filter_func\n",
    "            )\n",
    "            filter_time = time.time() - start_time\n",
    "            filter_times[label] = filter_time / num_queries\n",
    "            # total_requested = len(self.labels)\n",
    "            # matching_label = np.sum(self.labels == label)\n",
    "            # result_counts[label] = matching_label / total_requested\n",
    "            \n",
    "            # Filter Specificity\n",
    "            points_passing_filter = sum(filter_func(i) for i in range(self.num_elements))\n",
    "            filter_specificity[label] = points_passing_filter / self.num_elements\n",
    "            \n",
    "            # Accuracy Impact (Recall)\n",
    "            recall_scores[label] = self.calculate_recall(\n",
    "    filtered_labels, \n",
    "    unfiltered_labels,\n",
    "    query_points,\n",
    "    label,         \n",
    "    k            \n",
    ")\n",
    "\n",
    "        self.metrics['query_latency'] = {\n",
    "            'unfiltered': unfiltered_time / num_queries,\n",
    "            'filtered': filter_times\n",
    "        }\n",
    "        # self.metrics['filter_specificity'] = result_counts\n",
    "        self.metrics['filter_specificity'] = filter_specificity\n",
    "        self.metrics['recall_scores'] = recall_scores\n",
    "        self.metrics['filter_friction'] = {\n",
    "            'latency_overhead': {label: filter_times[label]/self.metrics['query_latency']['unfiltered'] \n",
    "                        for label in filter_times},\n",
    "            'specificity': filter_specificity,\n",
    "            'recall_impact': recall_scores\n",
    "}\n",
    "        \n",
    "        return self.metrics\n",
    "    \n",
    "    # def evaluate_query_performance(self, num_queries=100, k=10):\n",
    "    #     \"\"\"Evaluate query performance with different label filters\"\"\"\n",
    "    #     query_points = np.float32(np.random.random((num_queries, self.dim)))\n",
    "        \n",
    "    #     start_time = time.time()\n",
    "    #     unfiltered_labels, unfiltered_distances = self.index.knn_query(query_points, k=k, num_threads=1)\n",
    "    #     unfiltered_time = time.time() - start_time\n",
    "        \n",
    "    #     filter_times = {}\n",
    "    #     result_counts = {}\n",
    "    #     filter_specificity = {}\n",
    "\n",
    "        \n",
    "    #     for label in ['a', 'b', 'c']:\n",
    "    #         filter_func = self.create_label_filter(label)\n",
    "            \n",
    "    #         start_time = time.time()\n",
    "    #         filtered_labels, filtered_distances = self.index.knn_query(\n",
    "    #             query_points, k=k, num_threads=1, filter=filter_func\n",
    "    #         )\n",
    "    #         filter_time = time.time() - start_time\n",
    "            \n",
    "    #         filter_times[label] = filter_time / num_queries\n",
    "    #         total_requested = len(self.labels)\n",
    "    #         matching_label = np.sum(self.labels == label)\n",
    "    #         result_counts[label] = matching_label / total_requested\n",
    "    #         # Count how many points pass the filter\n",
    "    #         points_passing_filter = sum(filter_func(i) for i in range(self.num_elements))\n",
    "    #         filter_specificity[label] = points_passing_filter / self.num_elements\n",
    "        \n",
    "    #     self.metrics['query_latency'] = {\n",
    "    #         'unfiltered': unfiltered_time / num_queries,\n",
    "    #         'filtered': filter_times\n",
    "    #     }\n",
    "    #     # self.metrics['filter_specificity'] = result_counts\n",
    "\n",
    "    \n",
    "    #     self.metrics['filter_specificity'] = filter_specificity\n",
    "        \n",
    "    #     return self.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_evaluation_results(metrics):\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(f\"Build Time: {metrics['build_time']:.3f} seconds\")\n",
    "    \n",
    "    print(\"\\nLabel Distribution:\")\n",
    "    for label, freq in metrics['label_distribution'].items():\n",
    "        print(f\"- Label {label}: {freq*100:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nQuery Latency:\")\n",
    "    print(f\"- Unfiltered: {metrics['query_latency']['unfiltered']*1000:.2f} ms per query\")\n",
    "    for label, latency in metrics['query_latency']['filtered'].items():\n",
    "        print(f\"- Filtered (label {label}): {latency*1000:.2f} ms per query\")\n",
    "    \n",
    "    print(f\"\\nFilter Specificity:\")\n",
    "    for label, specificity in metrics['filter_specificity'].items():\n",
    "        print(f\"- Label {label}: {specificity*100:.1f}%\")\n",
    "        \n",
    "    print(\"\\nRecall Scores:\")\n",
    "    for label, recall in metrics['recall_scores'].items():\n",
    "        print(f\"- Label {label}: {recall*100:.1f}%\")\n",
    "        \n",
    "    print(\"\\nFilter Friction:\")\n",
    "    print(\"Latency Overhead:\")\n",
    "    for label, overhead in metrics['filter_friction']['latency_overhead'].items():\n",
    "        print(f\"- Label {label}: {overhead:.2f}x\")\n",
    "    print(\"Recall Impact:\")\n",
    "    for label, impact in metrics['filter_friction']['recall_impact'].items():\n",
    "        print(f\"- Label {label}: {impact*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_labeled_evaluation():\n",
    "    evaluator = LabelFilteredANNEvaluator()\n",
    "    \n",
    "    print(\"Generating labeled uniform data...\")\n",
    "    evaluator.generate_uniform_labeled_data()\n",
    "    \n",
    "    print(\"Building index...\")\n",
    "    evaluator.build_index()\n",
    "    \n",
    "    print(\"Evaluating performance...\")\n",
    "    metrics = evaluator.evaluate_query_performance()\n",
    "    \n",
    "    print_evaluation_results(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_labeled_evaluation1():\n",
    "    evaluator = LabelFilteredANNEvaluator1()\n",
    "    \n",
    "    print(\"Generating labeled skewed data...\")\n",
    "    evaluator.generate_skewed_labeled_data()\n",
    "    \n",
    "    print(\"Building index...\")\n",
    "    evaluator.build_index()\n",
    "    \n",
    "    print(\"Evaluating performance...\")\n",
    "    metrics = evaluator.evaluate_query_performance()\n",
    "    \n",
    "    print_evaluation_results(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating labeled uniform data...\n",
      "Building index...\n",
      "Evaluating performance...\n",
      "\n",
      "Evaluation Results:\n",
      "Build Time: 0.268 seconds\n",
      "\n",
      "Label Distribution:\n",
      "- Label a: 33.3%\n",
      "- Label b: 33.3%\n",
      "- Label c: 33.3%\n",
      "\n",
      "Query Latency:\n",
      "- Unfiltered: 0.02 ms per query\n",
      "- Filtered (label a): 0.18 ms per query\n",
      "- Filtered (label b): 0.16 ms per query\n",
      "- Filtered (label c): 0.21 ms per query\n",
      "\n",
      "Filter Specificity:\n",
      "- Label a: 33.3%\n",
      "- Label b: 33.3%\n",
      "- Label c: 33.3%\n",
      "\n",
      "Recall Scores:\n",
      "- Label a: 72.7%\n",
      "- Label b: 72.9%\n",
      "- Label c: 73.8%\n",
      "\n",
      "Filter Friction:\n",
      "Latency Overhead:\n",
      "- Label a: 9.00x\n",
      "- Label b: 8.00x\n",
      "- Label c: 10.50x\n",
      "Recall Impact:\n",
      "- Label a: 72.7%\n",
      "- Label b: 72.9%\n",
      "- Label c: 73.8%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_labeled_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating labeled skewed data...\n",
      "Building index...\n",
      "Evaluating performance...\n",
      "\n",
      "Evaluation Results:\n",
      "Build Time: 0.201 seconds\n",
      "\n",
      "Label Distribution:\n",
      "- Label a: 60.0%\n",
      "- Label b: 30.0%\n",
      "- Label c: 10.0%\n",
      "\n",
      "Query Latency:\n",
      "- Unfiltered: 0.06 ms per query\n",
      "- Filtered (label a): 0.11 ms per query\n",
      "- Filtered (label b): 0.24 ms per query\n",
      "- Filtered (label c): 0.39 ms per query\n",
      "\n",
      "Filter Specificity:\n",
      "- Label a: 60.0%\n",
      "- Label b: 30.0%\n",
      "- Label c: 10.0%\n",
      "\n",
      "Recall Scores:\n",
      "- Label a: 69.8%\n",
      "- Label b: 66.4%\n",
      "- Label c: 71.8%\n",
      "\n",
      "Filter Friction:\n",
      "Latency Overhead:\n",
      "- Label a: 1.84x\n",
      "- Label b: 4.00x\n",
      "- Label c: 6.50x\n",
      "Recall Impact:\n",
      "- Label a: 69.8%\n",
      "- Label b: 66.4%\n",
      "- Label c: 71.8%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_labeled_evaluation1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### HNSW Parameter Notes\n",
    "\n",
    "For the ef parameter, I set it bigger than k, which I found recommended in an article. While this increases build time,I also used cosine similarity as space metric (L2 norm showed similar results) it's worth noting that these parameters are considered in the benchmark of HNSW itself without filtering. Since my focus is on benchmarking the inline filtering, I just fixed these parameters for this implementation.\n",
    "\n",
    "These HNSW parameters primarily affect non-filtered search:\n",
    "\n",
    "ef_construction: higher → better recall, slower build\n",
    "\n",
    "ef: higher → better recall, slower search\n",
    "\n",
    "M: more connections → better recall, more memory usage\n",
    "\n",
    "Filtering Metrics:\n",
    "\n",
    "Specificity = (Number of points passing filter) / (Total number of points) (selectivity of the filter)\n",
    "\n",
    "Recall = |Retrieved True Nearest Neighbors ∩ Actual True Nearest Neighbors| / |Actual True Nearest Neighbors|\n",
    "\n",
    "Latency Overhead = Filtered Query Time / Unfiltered Query Time\n",
    "\n",
    "Warning: search with a filter works slow in python in multithreaded mode, therefore we set num_threads=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "The results show that, as expected, the query latency for unfiltered search (0.02ms) is significantly faster than inline filtered search (0.18-0.21ms) in the uniform distribution case. This is evident from:\n",
    "\n",
    "Query Latency:\n",
    "\n",
    "Unfiltered: 0.02ms\n",
    "Filtered: ranges from 0.18-0.21ms across labels\n",
    "\n",
    "\n",
    "Label Distribution and Specificity both show perfect uniform distribution:\n",
    "\n",
    "Each label (a, b, c) represents exactly 33.3% of data\n",
    "\n",
    "\n",
    "Performance Impact:\n",
    "\n",
    "Latency Overhead shows filtering is 9x-10x slower than unfiltered search\n",
    "Recall scores around 72% for all labels, showing consistent accuracy across uniformly distributed data\n",
    "\n",
    "\n",
    "\n",
    "The uniform distribution case demonstrates that filtering adds significant overhead but maintains consistent performance across labels due to their equal distribution.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The skewed distribution shows a notable pattern in performance impact:\n",
    "\n",
    "Query Latency varies significantly with label frequency:\n",
    "\n",
    "Label 'a' (60% of data): fastest filtered search at 0.11ms\n",
    "Label 'b' (30% of data): slower at 0.24ms\n",
    "Label 'c' (10% of data): slowest at 0.39ms\n",
    "Unfiltered remains fast at 0.06ms\n",
    "\n",
    "\n",
    "Latency Overhead shows inverse relationship with label frequency:\n",
    "\n",
    "Most frequent (label 'a'): lowest overhead at 1.84x\n",
    "Least frequent (label 'c'): highest overhead at 6.50x\n",
    "\n",
    "\n",
    "Interestingly, Recall scores remain relatively consistent (66-71%) despite skewed distribution, suggesting filtering maintains accuracy regardless of label frequency.\n",
    "\n",
    "This demonstrates that filtering performance is strongly influenced by label frequency, with rare labels incurring significantly higher latency overhead.\n",
    "\n",
    "\n",
    "#### Observation (relationship between label specificity and query latency in the inline filtering results):\n",
    "\n",
    "Label 'a':\n",
    "\n",
    "Uniform: 33.3% specificity → 0.18ms latency\n",
    "Skewed: 60.0% specificity → 0.11ms latency\n",
    "Shows faster performance with higher specificity\n",
    "\n",
    "\n",
    "Label 'c':\n",
    "\n",
    "Uniform: 33.3% specificity → 0.21ms latency\n",
    "Skewed: 10.0% specificity → 0.39ms latency\n",
    "Shows much slower performance with lower specificity\n",
    "\n",
    "\n",
    "\n",
    "This suggests that filter performance is better when searching for more frequent labels (higher specificity) and worse when searching for rare labels (lower specificity). The relationship appears to be inverse - as specificity decreases, latency increases significantly.\n",
    "\n",
    "\n",
    "Code source: https://github.com/nmslib/hnswlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlated Data using K means cluster \n",
    "\n",
    "This data is positively correlated as for HNSW it searches by distance the nearest k neighbors, so for the case of negative correlation we may inverse the search by multiplying by -1 if possible knowing that the HNSW index does not support that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelFilteredANNEvaluator2:\n",
    "    \"\"\"\n",
    "    Design Metrics for Filtered ANN Search:\n",
    "    1. Query Latency:\n",
    "       - Measures search time with/without filters\n",
    "       - Compares overhead of filtering\n",
    "    2. Accuracy Impact:\n",
    "       - Recall@k: proportion of true nearest neighbors found\n",
    "       - How filtering affects quality of results\n",
    "    3. Filter Friction:\n",
    "       - Filter specificity: proportion of points passing filter\n",
    "       - Impact of label distribution on performance\n",
    "    \"\"\"\n",
    "    def __init__(self, dim=16, num_elements=3000):\n",
    "        self.dim = dim\n",
    "        self.num_elements = num_elements\n",
    "        self.metrics = defaultdict(list)\n",
    "\n",
    "   \n",
    "    def generate_correlated_data(self, correlation_strength=0.7):\n",
    "        \"\"\"\n",
    "        Generate data where points with same labels are clustered together.\n",
    "        Args:\n",
    "            correlation_strength: Float between 0 and 1, controls cluster tightness\n",
    "        \"\"\"\n",
    "        # Generate cluster centers for each label\n",
    "        num_clusters = 3  # one per label\n",
    "        cluster_centers = np.random.random((num_clusters, self.dim))\n",
    "        \n",
    "        # Initialize data array\n",
    "        self.data = np.zeros((self.num_elements, self.dim), dtype=np.float32)\n",
    "        \n",
    "        # Generate points for each label/cluster\n",
    "        points_per_cluster = self.num_elements // num_clusters\n",
    "        self.labels = []\n",
    "        \n",
    "        for i, label in enumerate(['a', 'b', 'c']):\n",
    "            start_idx = i * points_per_cluster\n",
    "            end_idx = start_idx + points_per_cluster if i < 2 else self.num_elements\n",
    "            \n",
    "            # Generate random points\n",
    "            cluster_points = np.random.random((end_idx - start_idx, self.dim))\n",
    "            \n",
    "            # Move points closer to their cluster center\n",
    "            cluster_points = (1 - correlation_strength) * cluster_points + correlation_strength * cluster_centers[i]\n",
    "            \n",
    "            # Store points and labels\n",
    "            self.data[start_idx:end_idx] = cluster_points\n",
    "            self.labels.extend([label] * (end_idx - start_idx))\n",
    "        \n",
    "        self.labels = np.array(self.labels)\n",
    "        \n",
    "        # Shuffle data and labels together\n",
    "        p = np.random.permutation(len(self.data))\n",
    "        self.data = self.data[p]\n",
    "        self.labels = self.labels[p]\n",
    "        \n",
    "        # Calculate distribution\n",
    "        unique, counts = np.unique(self.labels, return_counts=True)\n",
    "        self.metrics['label_distribution'] = dict(zip(unique, counts / len(self.labels)))\n",
    "        \n",
    "        return self.data, self.labels\n",
    "    \n",
    "    def build_index(self):\n",
    "        \"\"\"Build HNSW index with the generated data\"\"\"\n",
    "        self.index = hnswlib.Index(space='cosine', dim=self.dim)\n",
    "        self.index.init_index(max_elements=self.num_elements, ef_construction=100, M=16)\n",
    "        self.index.set_ef(20)\n",
    "        self.index.set_num_threads(1)\n",
    "        start_time = time.time()\n",
    "        self.index.add_items(self.data, ids=np.arange(self.num_elements))\n",
    "        build_time = time.time() - start_time\n",
    "        self.metrics['build_time'] = build_time\n",
    "\n",
    "    def create_label_filter(self, target_label):\n",
    "        \"\"\"Create filter function for a specific label\"\"\"\n",
    "        def filter_function(idx):\n",
    "            return self.labels[idx] == target_label\n",
    "        return filter_function\n",
    "    \n",
    "    def calculate_recall(self, filtered_results, true_results, query_points, target_label, k):\n",
    "        \"\"\"\n",
    "    Calculate recall@k for filtered nearest neighbor search results.\n",
    "    \n",
    "    Args:\n",
    "        filtered_results: Results from filtered knn search (n_queries x k)\n",
    "        true_results: Results from unfiltered knn search (n_queries x k)\n",
    "        query_points: Query points used for search (n_queries x dim)\n",
    "        target_label: Label to filter for\n",
    "        k: Number of nearest neighbors\n",
    "    \n",
    "    Returns:\n",
    "        float: Average recall@k across all queries\n",
    "        \"\"\"\n",
    "        recall = 0\n",
    "        n_queries = len(query_points)\n",
    "    \n",
    "        target_mask = self.labels == target_label\n",
    "        target_data = self.data[target_mask]\n",
    "        target_indices = np.where(target_mask)[0]\n",
    "    \n",
    "        for i in range(n_queries):\n",
    "            distances = np.linalg.norm(target_data - query_points[i], axis=1)\n",
    "            true_neighbor_indices = target_indices[np.argsort(distances)[:k]]\n",
    "        \n",
    "            filtered_neighbor_indices = filtered_results[i]\n",
    "        \n",
    "            intersection = set(filtered_neighbor_indices) & set(true_neighbor_indices)\n",
    "            recall += len(intersection) / k\n",
    "    \n",
    "        return recall / n_queries\n",
    "\n",
    "    def evaluate_query_performance(self, num_queries=100, k=10):\n",
    "        \"\"\"Evaluate query performance with comprehensive metrics\"\"\"\n",
    "\n",
    "        query_points = np.float32(np.random.random((num_queries, self.dim)))\n",
    "        \n",
    "        # Unfiltered\n",
    "        start_time = time.time()\n",
    "        unfiltered_labels, unfiltered_distances = self.index.knn_query(query_points, k=k, num_threads=1)\n",
    "        unfiltered_time = time.time() - start_time\n",
    "        \n",
    "        filter_times = {}\n",
    "        recall_scores = {}\n",
    "        filter_specificity = {}\n",
    "        result_counts = {}\n",
    "        \n",
    "        # Per each label metrics\n",
    "        for label in ['a', 'b', 'c']:\n",
    "            filter_func = self.create_label_filter(label)\n",
    "            \n",
    "            # Latency\n",
    "            start_time = time.time()\n",
    "            filtered_labels, filtered_distances = self.index.knn_query(\n",
    "                query_points, k=k, num_threads=1, filter=filter_func\n",
    "            )\n",
    "            filter_time = time.time() - start_time\n",
    "            filter_times[label] = filter_time / num_queries\n",
    "            # total_requested = len(self.labels)\n",
    "            # matching_label = np.sum(self.labels == label)\n",
    "            # result_counts[label] = matching_label / total_requested\n",
    "            \n",
    "            # Filter Specificity\n",
    "            points_passing_filter = sum(filter_func(i) for i in range(self.num_elements))\n",
    "            filter_specificity[label] = points_passing_filter / self.num_elements\n",
    "            \n",
    "            # Accuracy Impact (Recall)\n",
    "            recall_scores[label] = self.calculate_recall(\n",
    "    filtered_labels, \n",
    "    unfiltered_labels,\n",
    "    query_points,\n",
    "    label,         \n",
    "    k            \n",
    ")\n",
    "\n",
    "        self.metrics['query_latency'] = {\n",
    "            'unfiltered': unfiltered_time / num_queries,\n",
    "            'filtered': filter_times\n",
    "        }\n",
    "        # self.metrics['filter_specificity'] = result_counts\n",
    "        self.metrics['filter_specificity'] = filter_specificity\n",
    "        self.metrics['recall_scores'] = recall_scores\n",
    "        self.metrics['filter_friction'] = {\n",
    "            'latency_overhead': {label: filter_times[label]/self.metrics['query_latency']['unfiltered'] \n",
    "                        for label in filter_times},\n",
    "            'specificity': filter_specificity,\n",
    "            'recall_impact': recall_scores\n",
    "}\n",
    "        \n",
    "        return self.metrics\n",
    "    \n",
    "    # def evaluate_query_performance(self, num_queries=100, k=10):\n",
    "    #     \"\"\"Evaluate query performance with different label filters\"\"\"\n",
    "    #     query_points = np.float32(np.random.random((num_queries, self.dim)))\n",
    "        \n",
    "    #     start_time = time.time()\n",
    "    #     unfiltered_labels, unfiltered_distances = self.index.knn_query(query_points, k=k, num_threads=1)\n",
    "    #     unfiltered_time = time.time() - start_time\n",
    "        \n",
    "    #     filter_times = {}\n",
    "    #     result_counts = {}\n",
    "    #     filter_specificity = {}\n",
    "\n",
    "        \n",
    "    #     for label in ['a', 'b', 'c']:\n",
    "    #         filter_func = self.create_label_filter(label)\n",
    "            \n",
    "    #         start_time = time.time()\n",
    "    #         filtered_labels, filtered_distances = self.index.knn_query(\n",
    "    #             query_points, k=k, num_threads=1, filter=filter_func\n",
    "    #         )\n",
    "    #         filter_time = time.time() - start_time\n",
    "            \n",
    "    #         filter_times[label] = filter_time / num_queries\n",
    "    #         total_requested = len(self.labels)\n",
    "    #         matching_label = np.sum(self.labels == label)\n",
    "    #         result_counts[label] = matching_label / total_requested\n",
    "    #         # Count how many points pass the filter\n",
    "    #         points_passing_filter = sum(filter_func(i) for i in range(self.num_elements))\n",
    "    #         filter_specificity[label] = points_passing_filter / self.num_elements\n",
    "        \n",
    "    #     self.metrics['query_latency'] = {\n",
    "    #         'unfiltered': unfiltered_time / num_queries,\n",
    "    #         'filtered': filter_times\n",
    "    #     }\n",
    "    #     # self.metrics['filter_specificity'] = result_counts\n",
    "\n",
    "    \n",
    "    #     self.metrics['filter_specificity'] = filter_specificity\n",
    "        \n",
    "    #     return self.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_evaluation_results(metrics):\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(f\"Build Time: {metrics['build_time']:.3f} seconds\")\n",
    "    \n",
    "    print(\"\\nLabel Distribution:\")\n",
    "    for label, freq in metrics['label_distribution'].items():\n",
    "        print(f\"- Label {label}: {freq*100:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nQuery Latency:\")\n",
    "    print(f\"- Unfiltered: {metrics['query_latency']['unfiltered']*1000:.2f} ms per query\")\n",
    "    for label, latency in metrics['query_latency']['filtered'].items():\n",
    "        print(f\"- Filtered (label {label}): {latency*1000:.2f} ms per query\")\n",
    "    \n",
    "    print(f\"\\nFilter Specificity:\")\n",
    "    for label, specificity in metrics['filter_specificity'].items():\n",
    "        print(f\"- Label {label}: {specificity*100:.1f}%\")\n",
    "        \n",
    "    print(\"\\nRecall Scores:\")\n",
    "    for label, recall in metrics['recall_scores'].items():\n",
    "        print(f\"- Label {label}: {recall*100:.1f}%\")\n",
    "        \n",
    "    print(\"\\nFilter Friction:\")\n",
    "    print(\"Latency Overhead:\")\n",
    "    for label, overhead in metrics['filter_friction']['latency_overhead'].items():\n",
    "        print(f\"- Label {label}: {overhead:.2f}x\")\n",
    "    print(\"Recall Impact:\")\n",
    "    for label, impact in metrics['filter_friction']['recall_impact'].items():\n",
    "        print(f\"- Label {label}: {impact*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_labeled_evaluation2():\n",
    "    evaluator = LabelFilteredANNEvaluator2()\n",
    "    \n",
    "    print(\"Generating labeled correlated data...\")\n",
    "    evaluator.generate_correlated_data()\n",
    "    \n",
    "    print(\"Building index...\")\n",
    "    evaluator.build_index()\n",
    "    \n",
    "    print(\"Evaluating performance...\")\n",
    "    metrics = evaluator.evaluate_query_performance()\n",
    "    \n",
    "    print_evaluation_results(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating labeled correlated data...\n",
      "Building index...\n",
      "Evaluating performance...\n",
      "\n",
      "Evaluation Results:\n",
      "Build Time: 0.686 seconds\n",
      "\n",
      "Label Distribution:\n",
      "- Label a: 33.3%\n",
      "- Label b: 33.3%\n",
      "- Label c: 33.3%\n",
      "\n",
      "Query Latency:\n",
      "- Unfiltered: 0.05 ms per query\n",
      "- Filtered (label a): 2.60 ms per query\n",
      "- Filtered (label b): 2.91 ms per query\n",
      "- Filtered (label c): 3.10 ms per query\n",
      "\n",
      "Filter Specificity:\n",
      "- Label a: 33.3%\n",
      "- Label b: 33.3%\n",
      "- Label c: 33.3%\n",
      "\n",
      "Recall Scores:\n",
      "- Label a: 75.9%\n",
      "- Label b: 70.7%\n",
      "- Label c: 72.2%\n",
      "\n",
      "Filter Friction:\n",
      "Latency Overhead:\n",
      "- Label a: 48.67x\n",
      "- Label b: 54.34x\n",
      "- Label c: 57.99x\n",
      "Recall Impact:\n",
      "- Label a: 75.9%\n",
      "- Label b: 70.7%\n",
      "- Label c: 72.2%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_labeled_evaluation2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "The results show that, as expected, the query latency for unfiltered search (0.05ms) is significantly faster than inline filtered search (2.60-3.10ms) in the correlated distribution case. \n",
    "The build time is slower for correlated due to clustered structure.\n",
    "\n",
    "Label Distribution and Specificity are the same as the uniform distribution.\n",
    "\n",
    "Each label (a, b, c) represents exactly 33.3% of data.\n",
    "\n",
    "\n",
    "Performance Impact:\n",
    "\n",
    "Latency Overhead shows filtering is 48x-57x slower than unfiltered search .This makes sense because clustered data changes how HNSW traverses the index.\n",
    "Recall scores around 70%-75% for all labels as it shows the impact of clustering on search accuracy.\n",
    "\n",
    "The correlated distribution case demonstrates that filtering adds significant overhead and \n",
    "shows a notable pattern in performance impact due to clustered structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelFilteredANNEvaluator3:\n",
    "    \"\"\"\n",
    "    Design Metrics for Filtered ANN Search:\n",
    "    1. Query Latency:\n",
    "       - Measures search time with/without filters\n",
    "       - Compares overhead of filtering\n",
    "    2. Accuracy Impact:\n",
    "       - Recall@k: proportion of true nearest neighbors found\n",
    "       - How filtering affects quality of results\n",
    "    3. Filter Friction:\n",
    "       - Filter specificity: proportion of points passing filter\n",
    "       - Impact of label distribution on performance\n",
    "    \"\"\"\n",
    "    def __init__(self, dim=16, num_elements=3000):\n",
    "        self.dim = dim\n",
    "        self.num_elements = num_elements\n",
    "        self.metrics = defaultdict(list)\n",
    "\n",
    "    def generate_all_a_data(self):\n",
    "        \"\"\"Generate data where all points have label 'a' (100% specificity)\"\"\"\n",
    "        self.data = np.float32(np.random.random((self.num_elements, self.dim)))\n",
    "        self.labels = np.array(['a'] * self.num_elements)\n",
    "    \n",
    "        unique, counts = np.unique(self.labels, return_counts=True)\n",
    "        self.metrics['label_distribution'] = dict(zip(unique, counts / len(self.labels)))\n",
    "    \n",
    "        return self.data, self.labels\n",
    " \n",
    "    def build_index(self):\n",
    "        \"\"\"Build HNSW index with the generated data\"\"\"\n",
    "        self.index = hnswlib.Index(space='cosine', dim=self.dim)\n",
    "        self.index.init_index(max_elements=self.num_elements, ef_construction=100, M=16)\n",
    "        self.index.set_ef(20)\n",
    "        self.index.set_num_threads(1)\n",
    "        start_time = time.time()\n",
    "        self.index.add_items(self.data, ids=np.arange(self.num_elements))\n",
    "        build_time = time.time() - start_time\n",
    "        self.metrics['build_time'] = build_time\n",
    "\n",
    "    def create_label_filter(self, target_label):\n",
    "        \"\"\"Create filter function for a specific label\"\"\"\n",
    "        def filter_function(idx):\n",
    "            return self.labels[idx] == target_label\n",
    "        return filter_function\n",
    "    \n",
    "    def calculate_recall(self, filtered_results, true_results, query_points, target_label, k):\n",
    "        \"\"\"\n",
    "    Calculate recall@k for filtered nearest neighbor search results.\n",
    "    \n",
    "    Args:\n",
    "        filtered_results: Results from filtered knn search (n_queries x k)\n",
    "        true_results: Results from unfiltered knn search (n_queries x k)\n",
    "        query_points: Query points used for search (n_queries x dim)\n",
    "        target_label: Label to filter for\n",
    "        k: Number of nearest neighbors\n",
    "    \n",
    "    Returns:\n",
    "        float: Average recall@k across all queries\n",
    "        \"\"\"\n",
    "        recall = 0\n",
    "        n_queries = len(query_points)\n",
    "    \n",
    "        target_mask = self.labels == target_label\n",
    "        target_data = self.data[target_mask]\n",
    "        target_indices = np.where(target_mask)[0]\n",
    "    \n",
    "        for i in range(n_queries):\n",
    "            distances = np.linalg.norm(target_data - query_points[i], axis=1)\n",
    "            true_neighbor_indices = target_indices[np.argsort(distances)[:k]]\n",
    "        \n",
    "            filtered_neighbor_indices = filtered_results[i]\n",
    "        \n",
    "            intersection = set(filtered_neighbor_indices) & set(true_neighbor_indices)\n",
    "            recall += len(intersection) / k\n",
    "    \n",
    "        return recall / n_queries\n",
    "\n",
    "    def evaluate_query_performance(self, num_queries=100, k=10):\n",
    "        \"\"\"Evaluate query performance with comprehensive metrics\"\"\"\n",
    "\n",
    "        query_points = np.float32(np.random.random((num_queries, self.dim)))\n",
    "        \n",
    "        #  Unfiltered\n",
    "        start_time = time.time()\n",
    "        unfiltered_labels, unfiltered_distances = self.index.knn_query(query_points, k=k, num_threads=1)\n",
    "        unfiltered_time = time.time() - start_time\n",
    "        \n",
    "        filter_times = {}\n",
    "        recall_scores = {}\n",
    "        filter_specificity = {}\n",
    "        result_counts = {}\n",
    "        \n",
    "        # Per each label metrics\n",
    "        for label in ['a']:\n",
    "            filter_func = self.create_label_filter(label)\n",
    "            \n",
    "            # Latency\n",
    "            start_time = time.time()\n",
    "            filtered_labels, filtered_distances = self.index.knn_query(\n",
    "                query_points, k=k, num_threads=1, filter=filter_func\n",
    "            )\n",
    "            filter_time = time.time() - start_time\n",
    "            filter_times[label] = filter_time / num_queries\n",
    "            # total_requested = len(self.labels)\n",
    "            # matching_label = np.sum(self.labels == label)\n",
    "            # result_counts[label] = matching_label / total_requested\n",
    "            \n",
    "            # Filter Specificity\n",
    "            points_passing_filter = sum(filter_func(i) for i in range(self.num_elements))\n",
    "            filter_specificity[label] = points_passing_filter / self.num_elements\n",
    "            \n",
    "            # Accuracy Impact (Recall)\n",
    "            recall_scores[label] = self.calculate_recall(\n",
    "    filtered_labels, \n",
    "    unfiltered_labels,\n",
    "    query_points, \n",
    "    label,         \n",
    "    k              \n",
    ")\n",
    "\n",
    "        self.metrics['query_latency'] = {\n",
    "            'unfiltered': unfiltered_time / num_queries,\n",
    "            'filtered': filter_times\n",
    "        }\n",
    "        # self.metrics['filter_specificity'] = result_counts\n",
    "        self.metrics['filter_specificity'] = filter_specificity\n",
    "        self.metrics['recall_scores'] = recall_scores\n",
    "        self.metrics['filter_friction'] = {\n",
    "            'latency_overhead': {label: filter_times[label]/self.metrics['query_latency']['unfiltered'] \n",
    "                        for label in filter_times},\n",
    "            'specificity': filter_specificity,\n",
    "            'recall_impact': recall_scores\n",
    "}\n",
    "        \n",
    "        return self.metrics\n",
    "    \n",
    "    # def evaluate_query_performance(self, num_queries=100, k=10):\n",
    "    #     \"\"\"Evaluate query performance with different label filters\"\"\"\n",
    "    #     query_points = np.float32(np.random.random((num_queries, self.dim)))\n",
    "        \n",
    "    #     start_time = time.time()\n",
    "    #     unfiltered_labels, unfiltered_distances = self.index.knn_query(query_points, k=k, num_threads=1)\n",
    "    #     unfiltered_time = time.time() - start_time\n",
    "        \n",
    "    #     filter_times = {}\n",
    "    #     result_counts = {}\n",
    "    #     filter_specificity = {}\n",
    "\n",
    "        \n",
    "    #     for label in ['a', 'b', 'c']:\n",
    "    #         filter_func = self.create_label_filter(label)\n",
    "            \n",
    "    #         start_time = time.time()\n",
    "    #         filtered_labels, filtered_distances = self.index.knn_query(\n",
    "    #             query_points, k=k, num_threads=1, filter=filter_func\n",
    "    #         )\n",
    "    #         filter_time = time.time() - start_time\n",
    "            \n",
    "    #         filter_times[label] = filter_time / num_queries\n",
    "    #         total_requested = len(self.labels)\n",
    "    #         matching_label = np.sum(self.labels == label)\n",
    "    #         result_counts[label] = matching_label / total_requested\n",
    "    #         # Count how many points pass the filter\n",
    "    #         points_passing_filter = sum(filter_func(i) for i in range(self.num_elements))\n",
    "    #         filter_specificity[label] = points_passing_filter / self.num_elements\n",
    "        \n",
    "    #     self.metrics['query_latency'] = {\n",
    "    #         'unfiltered': unfiltered_time / num_queries,\n",
    "    #         'filtered': filter_times\n",
    "    #     }\n",
    "    #     # self.metrics['filter_specificity'] = result_counts\n",
    "\n",
    "    \n",
    "    #     self.metrics['filter_specificity'] = filter_specificity\n",
    "        \n",
    "    #     return self.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_evaluation_results(metrics):\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(f\"Build Time: {metrics['build_time']:.3f} seconds\")\n",
    "    \n",
    "    print(\"\\nLabel Distribution:\")\n",
    "    for label, freq in metrics['label_distribution'].items():\n",
    "        print(f\"- Label {label}: {freq*100:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nQuery Latency:\")\n",
    "    print(f\"- Unfiltered: {metrics['query_latency']['unfiltered']*1000:.2f} ms per query\")\n",
    "    for label, latency in metrics['query_latency']['filtered'].items():\n",
    "        print(f\"- Filtered (label {label}): {latency*1000:.2f} ms per query\")\n",
    "    \n",
    "    print(f\"\\nFilter Specificity:\")\n",
    "    for label, specificity in metrics['filter_specificity'].items():\n",
    "        print(f\"- Label {label}: {specificity*100:.1f}%\")\n",
    "        \n",
    "    print(\"\\nRecall Scores:\")\n",
    "    for label, recall in metrics['recall_scores'].items():\n",
    "        print(f\"- Label {label}: {recall*100:.1f}%\")\n",
    "        \n",
    "    print(\"\\nFilter Friction:\")\n",
    "    print(\"Latency Overhead:\")\n",
    "    for label, overhead in metrics['filter_friction']['latency_overhead'].items():\n",
    "        print(f\"- Label {label}: {overhead:.2f}x\")\n",
    "    print(\"Recall Impact:\")\n",
    "    for label, impact in metrics['filter_friction']['recall_impact'].items():\n",
    "        print(f\"- Label {label}: {impact*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_labeled_evaluation3():\n",
    "    evaluator = LabelFilteredANNEvaluator3()\n",
    "    \n",
    "    print(\"Generating labeled data with only one label...\")\n",
    "    evaluator.generate_all_a_data()\n",
    "    \n",
    "    print(\"Building index...\")\n",
    "    evaluator.build_index()\n",
    "    \n",
    "    print(\"Evaluating performance...\")\n",
    "    metrics = evaluator.evaluate_query_performance()\n",
    "    \n",
    "    print_evaluation_results(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating labeled data with only one label...\n",
      "Building index...\n",
      "Evaluating performance...\n",
      "\n",
      "Evaluation Results:\n",
      "Build Time: 0.402 seconds\n",
      "\n",
      "Label Distribution:\n",
      "- Label a: 100.0%\n",
      "\n",
      "Query Latency:\n",
      "- Unfiltered: 0.05 ms per query\n",
      "- Filtered (label a): 0.14 ms per query\n",
      "\n",
      "Filter Specificity:\n",
      "- Label a: 100.0%\n",
      "\n",
      "Recall Scores:\n",
      "- Label a: 69.3%\n",
      "\n",
      "Filter Friction:\n",
      "Latency Overhead:\n",
      "- Label a: 2.96x\n",
      "Recall Impact:\n",
      "- Label a: 69.3%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_labeled_evaluation3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "The performance comparison shows the filter overhead even when all points have label 'a':\n",
    "\n",
    "Unfiltered search: 0.05ms\n",
    "Filtered search (100% label 'a'): 0.14ms\n",
    "\n",
    "2.96x overhead despite all points passing filter\n",
    "Better than 33.33% case but similar to 60% case\n",
    "Shows baseline filter overhead exists even with 100% passing\n",
    "\n",
    "\n",
    "\n",
    "This demonstrates that even when all points pass the filter, the mere presence of filtering logic adds latency compared to unfiltered search."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hnsw_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
