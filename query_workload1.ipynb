{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hnswlib\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelFilteredANNEvaluator:\n",
    "    \"\"\"\n",
    "    Design Metrics for Filtered ANN Search:\n",
    "    1. Query Latency:\n",
    "       - Measures search time with/without filters\n",
    "       - Compares overhead of filtering\n",
    "    2. Accuracy Impact:\n",
    "       - Recall@k: proportion of true nearest neighbors found\n",
    "       - How filtering affects quality of results\n",
    "    3. Filter Friction:\n",
    "       - Filter specificity: proportion of points passing filter\n",
    "       - Impact of label distribution on performance\n",
    "    \"\"\"\n",
    "    def __init__(self, dim=16, num_elements=3000):\n",
    "        self.dim = dim\n",
    "        self.num_elements = num_elements\n",
    "        self.metrics = defaultdict(list)\n",
    "\n",
    "    def generate_skewed_labeled_data(self):\n",
    "        \"\"\"Generate skewed data with three labels distributed as 60%, 30%, 10%\"\"\"\n",
    "        self.data = np.float32(np.random.random((self.num_elements, self.dim)))\n",
    "    \n",
    "        label_a_count = int(self.num_elements * 0.6)  # 60%\n",
    "        label_b_count = int(self.num_elements * 0.3)  # 30%\n",
    "        label_c_count = self.num_elements - label_a_count - label_b_count  # Remaining (10%)\n",
    "    \n",
    "        self.labels = np.array(['a'] * label_a_count + \n",
    "                          ['b'] * label_b_count + \n",
    "                          ['c'] * label_c_count)\n",
    "    \n",
    "        p = np.random.permutation(len(self.data))\n",
    "        self.data = self.data[p]\n",
    "        self.labels = self.labels[p]\n",
    "    \n",
    "        unique, counts = np.unique(self.labels, return_counts=True)\n",
    "        self.metrics['label_distribution'] = dict(zip(unique, counts / len(self.labels)))\n",
    "    \n",
    "        return self.data, self.labels\n",
    " \n",
    "    def build_index(self):\n",
    "        \"\"\"Build HNSW index with the generated data\"\"\"\n",
    "        self.index = hnswlib.Index(space='cosine', dim=self.dim)\n",
    "        self.index.init_index(max_elements=self.num_elements, ef_construction=100, M=16)\n",
    "        self.index.set_ef(20)\n",
    "        self.index.set_num_threads(1)\n",
    "        start_time = time.time()\n",
    "        self.index.add_items(self.data, ids=np.arange(self.num_elements))\n",
    "        build_time = time.time() - start_time\n",
    "        self.metrics['build_time'] = build_time\n",
    "\n",
    "    def create_label_filter(self, target_label):\n",
    "        \"\"\"Create filter function for a specific label\"\"\"\n",
    "        def filter_function(idx):\n",
    "            return self.labels[idx] == target_label\n",
    "        return filter_function\n",
    "    \n",
    "    def calculate_recall(self, filtered_results, true_results, query_points, target_label, k):\n",
    "        \"\"\"\n",
    "    Calculate recall@k for filtered nearest neighbor search results.\n",
    "    \n",
    "    Args:\n",
    "        filtered_results: Results from filtered knn search (n_queries x k)\n",
    "        true_results: Results from unfiltered knn search (n_queries x k)\n",
    "        query_points: Query points used for search (n_queries x dim)\n",
    "        target_label: Label to filter for\n",
    "        k: Number of nearest neighbors\n",
    "    \n",
    "    Returns:\n",
    "        float: Average recall@k across all queries\n",
    "        \"\"\"\n",
    "        recall = 0\n",
    "        n_queries = len(query_points)\n",
    "    \n",
    "        target_mask = self.labels == target_label\n",
    "        target_data = self.data[target_mask]\n",
    "        target_indices = np.where(target_mask)[0]\n",
    "    \n",
    "        for i in range(n_queries):\n",
    "            distances = np.linalg.norm(target_data - query_points[i], axis=1)\n",
    "            true_neighbor_indices = target_indices[np.argsort(distances)[:k]]\n",
    "        \n",
    "            filtered_neighbor_indices = filtered_results[i]\n",
    "        \n",
    "            intersection = set(filtered_neighbor_indices) & set(true_neighbor_indices)\n",
    "            recall += len(intersection) / k\n",
    "    \n",
    "        return recall / n_queries\n",
    "\n",
    "    def generate_distribution_aware_queries(self, num_queries=100):\n",
    "        \"\"\"\n",
    "        Generate random queries with varying filter specificities and attribute distributions\n",
    "        \"\"\"\n",
    "        queries = {}\n",
    "        \n",
    "        # High filter specificity (targeting common attributes)\n",
    "        label_counts = Counter(self.labels)\n",
    "        common_label = max(label_counts, key=label_counts.get)\n",
    "        queries['high_specificity'] = np.float32(\n",
    "            self.data[self.labels == common_label][\n",
    "                np.random.choice(np.sum(self.labels == common_label), num_queries)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Low filter specificity (targeting rare attributes)\n",
    "        rare_label = min(label_counts, key=label_counts.get)\n",
    "        queries['low_specificity'] = np.float32(\n",
    "            self.data[self.labels == rare_label][\n",
    "                np.random.choice(np.sum(self.labels == rare_label), num_queries)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Balanced distribution queries\n",
    "        balanced_indices = []\n",
    "        for label in np.unique(self.labels):\n",
    "            label_indices = np.where(self.labels == label)[0]\n",
    "            balanced_indices.extend(\n",
    "                np.random.choice(label_indices, num_queries // len(np.unique(self.labels)))\n",
    "            )\n",
    "        queries['balanced'] = np.float32(self.data[balanced_indices])\n",
    "        \n",
    "        return queries\n",
    "\n",
    "    def evaluate_query_performance(self, num_queries=100, k=10):\n",
    "        \"\"\"Evaluate query performance with comprehensive metrics including distribution awareness\"\"\"\n",
    "        \n",
    "        # Generate distribution-aware queries\n",
    "        query_sets = self.generate_distribution_aware_queries(num_queries)\n",
    "        all_metrics = {}\n",
    "        \n",
    "        for query_type, query_points in query_sets.items():\n",
    "            metrics = defaultdict(dict)\n",
    "            \n",
    "            # Unfiltered baseline\n",
    "            start_time = time.time()\n",
    "            unfiltered_labels, unfiltered_distances = self.index.knn_query(query_points, k=k, num_threads=1)\n",
    "            unfiltered_time = time.time() - start_time\n",
    "            \n",
    "            filter_times = {}\n",
    "            recall_scores = {}\n",
    "            filter_specificity = {}\n",
    "            \n",
    "            # Per each label metrics\n",
    "            for label in ['a', 'b', 'c']:\n",
    "                filter_func = self.create_label_filter(label)\n",
    "                \n",
    "                # Latency\n",
    "                start_time = time.time()\n",
    "                filtered_labels, filtered_distances = self.index.knn_query(\n",
    "                    query_points, k=k, num_threads=1, filter=filter_func\n",
    "                )\n",
    "                filter_time = time.time() - start_time\n",
    "                filter_times[label] = filter_time / num_queries\n",
    "                \n",
    "                # Filter Specificity\n",
    "                points_passing_filter = sum(filter_func(i) for i in range(self.num_elements))\n",
    "                filter_specificity[label] = points_passing_filter / self.num_elements\n",
    "                \n",
    "                # Accuracy Impact (Recall)\n",
    "                recall_scores[label] = self.calculate_recall(\n",
    "                    filtered_labels,\n",
    "                    unfiltered_labels,\n",
    "                    query_points,\n",
    "                    label,\n",
    "                    k\n",
    "                )\n",
    "            \n",
    "            metrics['query_latency'] = {\n",
    "                'unfiltered': unfiltered_time / num_queries,\n",
    "                'filtered': filter_times\n",
    "            }\n",
    "            metrics['filter_specificity'] = filter_specificity\n",
    "            metrics['recall_scores'] = recall_scores\n",
    "            metrics['filter_friction'] = {\n",
    "                'latency_overhead': {label: filter_times[label]/metrics['query_latency']['unfiltered'] \n",
    "                            for label in filter_times},\n",
    "                'specificity': filter_specificity,\n",
    "                'recall_impact': recall_scores\n",
    "            }\n",
    "            \n",
    "            all_metrics[query_type] = metrics\n",
    "        \n",
    "        self.metrics['distribution_aware'] = all_metrics\n",
    "        return all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating labeled skewed data...\n",
      "\n",
      "Label Distribution:\n",
      "- Label a: 60.0%\n",
      "- Label b: 30.0%\n",
      "- Label c: 10.0%\n",
      "\n",
      "Building index...\n",
      "Build Time: 0.748 seconds\n",
      "\n",
      "Evaluating performance...\n",
      "\n",
      "Distribution-Aware Evaluation Results:\n",
      "\n",
      "=== high_specificity Queries ===\n",
      "\n",
      "Query Latency:\n",
      "- Unfiltered: 0.13 ms per query\n",
      "- Filtered (label a): 0.39 ms per query\n",
      "- Filtered (label b): 0.62 ms per query\n",
      "- Filtered (label c): 1.85 ms per query\n",
      "\n",
      "Filter Specificity:\n",
      "- Label a: 60.0%\n",
      "- Label b: 30.0%\n",
      "- Label c: 10.0%\n",
      "\n",
      "Recall Scores:\n",
      "- Label a: 74.9%\n",
      "- Label b: 71.3%\n",
      "- Label c: 72.8%\n",
      "\n",
      "Filter Friction:\n",
      "Latency Overhead:\n",
      "- Label a: 2.89x\n",
      "- Label b: 4.61x\n",
      "- Label c: 13.71x\n",
      "Recall Impact:\n",
      "- Label a: 74.9%\n",
      "- Label b: 71.3%\n",
      "- Label c: 72.8%\n",
      "\n",
      "=== low_specificity Queries ===\n",
      "\n",
      "Query Latency:\n",
      "- Unfiltered: 0.12 ms per query\n",
      "- Filtered (label a): 0.42 ms per query\n",
      "- Filtered (label b): 0.82 ms per query\n",
      "- Filtered (label c): 1.79 ms per query\n",
      "\n",
      "Filter Specificity:\n",
      "- Label a: 60.0%\n",
      "- Label b: 30.0%\n",
      "- Label c: 10.0%\n",
      "\n",
      "Recall Scores:\n",
      "- Label a: 69.0%\n",
      "- Label b: 67.6%\n",
      "- Label c: 75.0%\n",
      "\n",
      "Filter Friction:\n",
      "Latency Overhead:\n",
      "- Label a: 3.39x\n",
      "- Label b: 6.61x\n",
      "- Label c: 14.41x\n",
      "Recall Impact:\n",
      "- Label a: 69.0%\n",
      "- Label b: 67.6%\n",
      "- Label c: 75.0%\n",
      "\n",
      "=== balanced Queries ===\n",
      "\n",
      "Query Latency:\n",
      "- Unfiltered: 0.14 ms per query\n",
      "- Filtered (label a): 0.27 ms per query\n",
      "- Filtered (label b): 0.33 ms per query\n",
      "- Filtered (label c): 0.75 ms per query\n",
      "\n",
      "Filter Specificity:\n",
      "- Label a: 60.0%\n",
      "- Label b: 30.0%\n",
      "- Label c: 10.0%\n",
      "\n",
      "Recall Scores:\n",
      "- Label a: 68.9%\n",
      "- Label b: 70.6%\n",
      "- Label c: 74.8%\n",
      "\n",
      "Filter Friction:\n",
      "Latency Overhead:\n",
      "- Label a: 1.92x\n",
      "- Label b: 2.35x\n",
      "- Label c: 5.30x\n",
      "Recall Impact:\n",
      "- Label a: 68.9%\n",
      "- Label b: 70.6%\n",
      "- Label c: 74.8%\n"
     ]
    }
   ],
   "source": [
    "def print_evaluation_results(metrics):\n",
    "    print(\"\\nDistribution-Aware Evaluation Results:\")\n",
    "    \n",
    "    # Print metrics for each query type\n",
    "    for query_type, query_metrics in metrics.items():\n",
    "        print(f\"\\n=== {query_type} Queries ===\")\n",
    "        \n",
    "        print(f\"\\nQuery Latency:\")\n",
    "        print(f\"- Unfiltered: {query_metrics['query_latency']['unfiltered']*1000:.2f} ms per query\")\n",
    "        for label, latency in query_metrics['query_latency']['filtered'].items():\n",
    "            print(f\"- Filtered (label {label}): {latency*1000:.2f} ms per query\")\n",
    "        \n",
    "        print(f\"\\nFilter Specificity:\")\n",
    "        for label, specificity in query_metrics['filter_specificity'].items():\n",
    "            print(f\"- Label {label}: {specificity*100:.1f}%\")\n",
    "        \n",
    "        print(\"\\nRecall Scores:\")\n",
    "        for label, recall in query_metrics['recall_scores'].items():\n",
    "            print(f\"- Label {label}: {recall*100:.1f}%\")\n",
    "        \n",
    "        print(\"\\nFilter Friction:\")\n",
    "        print(\"Latency Overhead:\")\n",
    "        for label, overhead in query_metrics['filter_friction']['latency_overhead'].items():\n",
    "            print(f\"- Label {label}: {overhead:.2f}x\")\n",
    "        print(\"Recall Impact:\")\n",
    "        for label, impact in query_metrics['filter_friction']['recall_impact'].items():\n",
    "            print(f\"- Label {label}: {impact*100:.1f}%\")\n",
    "\n",
    "def run_labeled_evaluation():\n",
    "    evaluator = LabelFilteredANNEvaluator()\n",
    "    \n",
    "    print(\"Generating labeled skewed data...\")\n",
    "    evaluator.generate_skewed_labeled_data()\n",
    "    \n",
    "    print(\"\\nLabel Distribution:\")\n",
    "    for label, freq in evaluator.metrics['label_distribution'].items():\n",
    "        print(f\"- Label {label}: {freq*100:.1f}%\")\n",
    "    \n",
    "    print(\"\\nBuilding index...\")\n",
    "    evaluator.build_index()\n",
    "    print(f\"Build Time: {evaluator.metrics['build_time']:.3f} seconds\")\n",
    "    \n",
    "    print(\"\\nEvaluating performance...\")\n",
    "    metrics = evaluator.evaluate_query_performance()\n",
    "    \n",
    "    print_evaluation_results(metrics)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_labeled_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Types in Skewed Distribution:\n",
    "Skewed Distribution (60%, 30%, 10%)\n",
    "\n",
    "High Specificity: 100 queries from most common label (a) (Queries targeting common labels)\n",
    "\n",
    "Low Specificity: 100 queries from rarest label (c) (Queries targeting rare labels)\n",
    "\n",
    "Balanced: ~33 queries from each label (a,b,c) (Queries evenly distributed across all labels)\n",
    "\n",
    "For uniform Distribution (33%, 33%, 33%):\n",
    "\n",
    "Can use random queries since all labels have equal distribution\n",
    "No meaningful distinction between high/low specificity\n",
    "Balanced sampling happens naturally (as in the previous notebook)\n",
    "\n",
    "Note: All queries are random within their category, but sampling is controlled based on label frequencies in skewed case.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "\n",
    "Query Types Impact:\n",
    "\n",
    "\n",
    "high_specificity: queries from label 'a' (most common)\n",
    "low_specificity: queries from label 'c' (rarest)\n",
    "balanced: equal mix of all labels\n",
    "\n",
    "\n",
    "Key Observations:\n",
    "\n",
    "\n",
    "Latency overhead increases as label rarity increases (c > b > a)\n",
    "Label 'c' (10%) has highest latency overhead (5-14x slower)\n",
    "Label 'a' (60%) has lowest latency overhead (2-3x slower)\n",
    "Recall stays fairly consistent (~70-75%) across all scenarios\n",
    "\n",
    "\n",
    "Query Workload Implications:\n",
    "\n",
    "\n",
    "Filtering for rare labels (c) is most expensive\n",
    "Balanced query workload shows best overall performance\n",
    "High-specificity queries (from common label 'a') don't necessarily give better recall\n",
    "\n",
    "The main takeaway is that query performance significantly depends on label distribution - searching for rare labels takes longer but doesn't affect accuracy much.\n",
    "\n",
    "\n",
    "The pattern shows: as specificity decreases (60% → 30% → 10%), query latency increases. This makes sense because finding k neighbors in a smaller subset of points requires checking more total points to find enough valid ones. However, balanced queries show lower overall latencies, suggesting query source distribution affects performance.\n",
    "\n",
    "Another pattern: For each label (regardless of its specificity), balanced queries show significantly better latency. The source of the query (high vs low specificity) doesn't impact latency as much as the query distribution type. Specificity still matters - rarer labels (c) take longer across all query types."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hnsw_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
